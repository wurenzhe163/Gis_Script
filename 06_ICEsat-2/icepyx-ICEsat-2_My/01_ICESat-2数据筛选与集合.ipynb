{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import geopandas as gpd\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.geometry import Point\n",
    "from PackageDeepLearn.utils import file_search_wash as fsw\n",
    "\n",
    "# 所有条带\n",
    "sub_file_list = ['gt1l/', 'gt1r/', 'gt2l/', 'gt2r/', 'gt3l/', 'gt3r/']\n",
    "start_time = time.time()\n",
    "addnum = 0\n",
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2数据\\ATL_03', '.h5')[0:65]\n",
    "\n",
    "shapefile_path = r\"D:\\BaiduSyncdisk\\02_论文相关\\在写\\SAM冰湖\\数据\\2023_05_31_to_2023_09_15_样本修正.shp\"\n",
    "gdf_polygons = gpd.read_file(shapefile_path)\n",
    "\n",
    "# 假设原始的 CRS 是 EPSG:4326\n",
    "original_crs = gdf_polygons.crs\n",
    "\n",
    "# 转换为适当的投影坐标系，例如 EPSG:32633\n",
    "projected_gdf = gdf_polygons.to_crs(epsg=32633)\n",
    "\n",
    "# 应用100米的缓冲区\n",
    "projected_gdf['geometry'] = projected_gdf.geometry.buffer(50)\n",
    "\n",
    "# 将结果转换回原始的地理坐标系\n",
    "gdf_polygons_buffered = projected_gdf.to_crs(original_crs)\n",
    "\n",
    "def process_spatial_join(gdf_batch, gdf_polygons_buffered):\n",
    "    # 执行空间连接\n",
    "    joined_df = gpd.sjoin(gdf_batch, gdf_polygons_buffered, how='left', predicate='intersects')\n",
    "\n",
    "    # 确保只有那些与目标几何体相交的条目被保留\n",
    "    if 'index_right' in joined_df.columns:\n",
    "        joined_df = joined_df.dropna(subset=['index_right'])\n",
    "\n",
    "    return joined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_03 Old\n",
    "节省内存，但是运算效率低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建空间索引\n",
    "polygon_tree = STRtree(gdf_polygons_buffered.geometry)\n",
    "\n",
    "def filter_data_by_polygon(df, polygon_tree, batch_size=10000):\n",
    "    '''求取SETP包含的激光点'''\n",
    "    filtered_data_list = []\n",
    "    for start in trange(0, len(df), batch_size):\n",
    "        end = min(start + batch_size, len(df))\n",
    "        batch_df = df.iloc[start:end].copy()\n",
    "        batch_df['geometry'] = gpd.points_from_xy(batch_df['lon'], batch_df['lat'])\n",
    "        gdf_points = gpd.GeoDataFrame(batch_df, geometry='geometry')\n",
    "        \n",
    "        # 利用空间索引进行过滤\n",
    "        possible_matches_index = polygon_tree.query(gdf_points.geometry, predicate='intersects')\n",
    "        possible_matches_index = [item for sublist in possible_matches_index for item in sublist]  # 展平数组\n",
    "        if possible_matches_index:\n",
    "            possible_matches = gdf_polygons_buffered.iloc[possible_matches_index]\n",
    "            print('length of possible_matches = {}'.format(len(possible_matches)))\n",
    "            precise_matches = gdf_points[gdf_points.geometry.apply(lambda x: possible_matches.contains(x).any())]\n",
    "\n",
    "            if not precise_matches.empty:\n",
    "                precise_matches = precise_matches.drop(columns='geometry')\n",
    "                filtered_data_list.append(precise_matches)\n",
    "    \n",
    "    if filtered_data_list:\n",
    "        filtered_data = pd.concat(filtered_data_list, ignore_index=True)\n",
    "        return filtered_data\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# 提取所需信息，形成数据文件\n",
    "for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "    combined_data = pd.DataFrame()\n",
    "    data = h5py.File(file_path, 'r')\n",
    "    output_file_path = r'E:\\SETP_ICESat-2数据\\ATL_03\\ATL03_SETPGL_ALL_{}.h5'.format(idx + addnum)\n",
    "    if os.path.exists(output_file_path):\n",
    "        print('{} 存在，跳过'.format(output_file_path))\n",
    "        continue\n",
    "    for subgroup in tqdm(sub_file_list, desc=\"Processing Subgroups\", leave=False):\n",
    "        if subgroup in data:\n",
    "            time_data = data.get(os.path.join(subgroup, 'heights/delta_time'))\n",
    "            lat = data.get(os.path.join(subgroup, 'heights/lat_ph'))\n",
    "            lon = data.get(os.path.join(subgroup, 'heights/lon_ph'))\n",
    "            dist_ph_along = data.get(os.path.join(subgroup, 'heights/dist_ph_along'))\n",
    "            height = data.get(os.path.join(subgroup, 'heights/h_ph'))\n",
    "            signal_conf_ph = data.get(os.path.join(subgroup, 'heights/signal_conf_ph'))\n",
    "            quality_ph = data.get(os.path.join(subgroup, 'heights/quality_ph'))\n",
    "\n",
    "            if all(x is not None for x in [lat, lon, height, time_data, dist_ph_along, quality_ph, signal_conf_ph]):\n",
    "                df = pd.DataFrame(data={\n",
    "                    'time': time_data[:],\n",
    "                    'lat': lat[:],\n",
    "                    'lon': lon[:],\n",
    "                    'dist_ph_along': dist_ph_along[:],\n",
    "                    'height': height[:],\n",
    "                    'quality_ph': quality_ph[:],\n",
    "                    'signal_conf_ph_1': signal_conf_ph[:, 0],\n",
    "                    'signal_conf_ph_2': signal_conf_ph[:, 1],\n",
    "                    'signal_conf_ph_3': signal_conf_ph[:, 2],\n",
    "                    'signal_conf_ph_4': signal_conf_ph[:, 3],\n",
    "                    'signal_conf_ph_5': signal_conf_ph[:, 4]\n",
    "                })\n",
    "                df['subgroup'] = subgroup\n",
    "                \n",
    "                # 过滤数据，删除 signal_conf_ph_1、signal_conf_ph_2、signal_conf_ph_3、signal_conf_ph_4、signal_conf_ph_5 小于 0 的数据\n",
    "                df = df[(df['signal_conf_ph_1'] >= 0) |\n",
    "                        (df['signal_conf_ph_2'] >= 0) |\n",
    "                        (df['signal_conf_ph_3'] >= 0) |\n",
    "                        (df['signal_conf_ph_4'] >= 0) |\n",
    "                        (df['signal_conf_ph_5'] >= 0)]\n",
    "                \n",
    "                # 过滤数据，根据多边形的包含关系\n",
    "                filtered_df = filter_data_by_polygon(df, polygon_tree)\n",
    "                \n",
    "                if not filtered_df.empty:\n",
    "                    # 连接属性表信息\n",
    "                    gdf_filtered = gpd.GeoDataFrame(filtered_df, geometry=gpd.points_from_xy(filtered_df['lon'], filtered_df['lat']), crs=original_crs)\n",
    "                    # 将多边形的属性（除去 'geometry' 列）连接到点数据中\n",
    "                    joined_df = gpd.sjoin(gdf_filtered, gdf_polygons_buffered[['Sort', '关系', 'Area_pre', 'geometry']], how='left', predicate='intersects')\n",
    "                    \n",
    "                    if not joined_df.empty:\n",
    "                        # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                        joined_df = joined_df.drop(columns='geometry')\n",
    "                        combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "    data.close()\n",
    "\n",
    "    if not combined_data.empty:\n",
    "        combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_03 \n",
    "属性表直接连接筛选，快速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取所需信息，形成数据文件\n",
    "for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "    combined_data = pd.DataFrame()\n",
    "    data = h5py.File(file_path, 'r')\n",
    "    output_file_path = r'E:\\SETP_ICESat-2数据\\ATL_03\\ATL03_SETPGL_ALL_{}.h5'.format(idx + addnum)\n",
    "    if os.path.exists(output_file_path):\n",
    "        print('{} 存在，跳过'.format(output_file_path))\n",
    "        continue\n",
    "    for subgroup in tqdm(sub_file_list, desc=\"Processing Subgroups\", leave=False):\n",
    "        if subgroup in data:\n",
    "            time_data = data.get(os.path.join(subgroup, 'heights/delta_time'))\n",
    "            lat = data.get(os.path.join(subgroup, 'heights/lat_ph'))\n",
    "            lon = data.get(os.path.join(subgroup, 'heights/lon_ph'))\n",
    "            dist_ph_along = data.get(os.path.join(subgroup, 'heights/dist_ph_along'))\n",
    "            height = data.get(os.path.join(subgroup, 'heights/h_ph'))\n",
    "            signal_conf_ph = data.get(os.path.join(subgroup, 'heights/signal_conf_ph'))\n",
    "            quality_ph = data.get(os.path.join(subgroup, 'heights/quality_ph'))\n",
    "\n",
    "            if all(x is not None for x in [lat, lon, height, time_data, dist_ph_along, quality_ph, signal_conf_ph]):\n",
    "                df = pd.DataFrame(data={\n",
    "                    'time': time_data[:],\n",
    "                    'lat': lat[:],\n",
    "                    'lon': lon[:],\n",
    "                    'dist_ph_along': dist_ph_along[:],\n",
    "                    'height': height[:],\n",
    "                    'quality_ph': quality_ph[:],\n",
    "                    'signal_conf_ph_1': signal_conf_ph[:, 0],\n",
    "                    'signal_conf_ph_2': signal_conf_ph[:, 1],\n",
    "                    'signal_conf_ph_3': signal_conf_ph[:, 2],\n",
    "                    'signal_conf_ph_4': signal_conf_ph[:, 3],\n",
    "                    'signal_conf_ph_5': signal_conf_ph[:, 4]\n",
    "                })\n",
    "                df['subgroup'] = subgroup\n",
    "                \n",
    "                # 过滤数据，删除 signal_conf_ph_1、signal_conf_ph_2、signal_conf_ph_3、signal_conf_ph_4、signal_conf_ph_5 小于 0 的数据\n",
    "                df = df[(df['signal_conf_ph_1'] >= 0) |\n",
    "                        (df['signal_conf_ph_2'] >= 0) |\n",
    "                        (df['signal_conf_ph_3'] >= 0) |\n",
    "                        (df['signal_conf_ph_4'] >= 0) |\n",
    "                        (df['signal_conf_ph_5'] >= 0)]\n",
    "                \n",
    "\n",
    "                if not df.empty:\n",
    "                    # 连接属性表信息\n",
    "                    gdf_filtered = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=original_crs)\n",
    "                    \n",
    "                    batch_size = 10000\n",
    "                    n_batches = (len(gdf_filtered) + batch_size - 1) // batch_size  # 计算总批次数\n",
    "                    result = []\n",
    "\n",
    "                    for n in tqdm(range(n_batches), desc=\"Processing batches\"):\n",
    "                        gdf_batch = gdf_filtered[n * batch_size:(n + 1) * batch_size]\n",
    "                        joined_df = process_spatial_join(gdf_batch, gdf_polygons_buffered)\n",
    "                        if len(joined_df)>0:\n",
    "                            pass\n",
    "\n",
    "                        if not joined_df.empty:\n",
    "                            # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                            joined_df = joined_df.drop(columns='geometry')\n",
    "                            combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "    data.close()\n",
    "\n",
    "    if not combined_data.empty:\n",
    "        combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2数据\\ATL_06_Landice', '.h5')\n",
    "ATL06_NoData = []\n",
    "# 提取所需信息，形成数据文件\n",
    "try:\n",
    "    for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "        combined_data = pd.DataFrame()\n",
    "        data = h5py.File(file_path, 'r')\n",
    "        output_file_path = r'E:\\SETP_ICESat-2数据\\ATL_06_Landice\\ATL06_ALL\\{}_{}.h5'.format(os.path.basename(file_path).split('.')[0],idx + addnum)\n",
    "        if os.path.exists(output_file_path):\n",
    "            print('{} 存在，跳过'.format(output_file_path))\n",
    "            continue\n",
    "        for subgroup in tqdm(sub_file_list, desc=\"Processing Subgroups\", leave=False):\n",
    "            if subgroup in data:\n",
    "                time_data = data.get(os.path.join(subgroup, 'land_ice_segments/delta_time'))\n",
    "                lat = data.get(os.path.join(subgroup, 'land_ice_segments/latitude'))\n",
    "                lon = data.get(os.path.join(subgroup, 'land_ice_segments/longitude'))\n",
    "                height = data.get(os.path.join(subgroup, 'land_ice_segments/h_li'))  \n",
    "                dem = data.get(os.path.join(subgroup, 'land_ice_segments/dem/dem_h'))\n",
    "                signal_conf_ph = data.get(os.path.join(subgroup, 'land_ice_segments/atl06_quality_summary')) #(Meanings: [0 1]) (Values: ['best_quality', 'potential_problem'])\n",
    "                id = data.get(os.path.join(subgroup, 'land_ice_segments/segment_id'))      \n",
    "\n",
    "                if all(x is not None for x in [time_data, lat, lon, height,dem, signal_conf_ph ,id]):\n",
    "                    df = pd.DataFrame(data={\n",
    "                        'time': time_data[:],\n",
    "                        'lat': lat[:],\n",
    "                        'lon': lon[:],\n",
    "                        'height': height[:],\n",
    "                        'dem':dem,\n",
    "                        'signal_conf_ph':signal_conf_ph[:],\n",
    "                        'id':id[:],\n",
    "                    })\n",
    "                    df['subgroup'] = subgroup\n",
    "\n",
    "                    if not df.empty:\n",
    "                        # 连接属性表信息\n",
    "                        gdf_filtered = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=original_crs)\n",
    "                        \n",
    "                        batch_size = 10000\n",
    "                        n_batches = (len(gdf_filtered) + batch_size - 1) // batch_size  # 计算总批次数\n",
    "                        result = []\n",
    "\n",
    "                        for n in tqdm(range(n_batches), desc=\"Processing batches\"):\n",
    "                            gdf_batch = gdf_filtered[n * batch_size:(n + 1) * batch_size]\n",
    "                            joined_df = process_spatial_join(gdf_batch, gdf_polygons_buffered)\n",
    "                            if len(joined_df)>0:\n",
    "                                pass\n",
    "\n",
    "                            if not joined_df.empty:\n",
    "                                # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                                joined_df = joined_df.drop(columns='geometry')\n",
    "                                combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "        data.close()\n",
    "\n",
    "        if not combined_data.empty:\n",
    "            combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "        else:\n",
    "            print(f\"No data found in {file_path}\")\n",
    "            ATL06_NoData.append(file_path)\n",
    "\n",
    "except:\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2数据\\ATL_07_Seaice', '.h5')\n",
    "ATL07_NoData = []\n",
    "# 提取所需信息，形成数据文件\n",
    "for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "    combined_data = pd.DataFrame()\n",
    "    data = h5py.File(file_path, 'r')\n",
    "    output_file_path = r'E:\\SETP_ICESat-2数据\\ATL_07_Seaice\\ATL07_ALL\\{}_{}.h5'.format(os.path.basename(file_path).split('.')[0],idx + addnum)\n",
    "    if os.path.exists(output_file_path):\n",
    "        print('{} 存在，跳过'.format(output_file_path))\n",
    "        continue\n",
    "    for subgroup in sub_file_list:\n",
    "        if subgroup in data:\n",
    "            time_data = data.get(os.path.join(subgroup, 'sea_ice_segments/delta_time'))\n",
    "            lat = data.get(os.path.join(subgroup, 'sea_ice_segments/latitude'))\n",
    "            lon = data.get(os.path.join(subgroup, 'sea_ice_segments/longitude'))\n",
    "            height = data.get(os.path.join(subgroup, 'sea_ice_segments/heights/height_segment_height'))\n",
    "            dem = data.get(os.path.join(subgroup, 'dem/dem_h'))\n",
    "            id = data.get(os.path.join(subgroup, 'sea_ice_segments/height_segment_id'))\n",
    "            signal_conf_ph = data.get(os.path.join(subgroup, 'sea_ice_segments/heights/height_segment_quality'))# (Meanings: [0 1]) (Values: ['bad_quality', 'good_quality'])\n",
    "\n",
    "            if all(x is not None for x in [time_data, lat, lon, height, dem, signal_conf_ph ,id]):\n",
    "                print([len(i) for i in [time_data, lat, lon, height, dem, signal_conf_ph ,id]])\n",
    "                df = pd.DataFrame(data={\n",
    "                    'time': time_data[:],\n",
    "                    'lat': lat[:],\n",
    "                    'lon': lon[:],\n",
    "                    'height': height[:],\n",
    "                    'dem': dem[:],\n",
    "                    'signal_conf_ph':signal_conf_ph[:],\n",
    "                    'id':id[:],\n",
    "                })\n",
    "                df['subgroup'] = subgroup\n",
    "\n",
    "                if not df.empty:\n",
    "                    # 连接属性表信息\n",
    "                    gdf_filtered = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=original_crs)\n",
    "                    \n",
    "                    batch_size = 10000\n",
    "                    n_batches = (len(gdf_filtered) + batch_size - 1) // batch_size  # 计算总批次数\n",
    "                    result = []\n",
    "\n",
    "                    for n in range(n_batches):\n",
    "                        gdf_batch = gdf_filtered[n * batch_size:(n + 1) * batch_size]\n",
    "                        joined_df = process_spatial_join(gdf_batch, gdf_polygons_buffered)\n",
    "                        if len(joined_df)>0:\n",
    "                            pass\n",
    "\n",
    "                        if not joined_df.empty:\n",
    "                            # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                            joined_df = joined_df.drop(columns='geometry')\n",
    "                            combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "    data.close()\n",
    "\n",
    "    if not combined_data.empty:\n",
    "        combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "    else:\n",
    "        print(f\"No data found in {file_path}\")\n",
    "        ATL07_NoData.append(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2数据\\ATL_08_LandVegetation', '.h5')\n",
    "ATL08_NoData = []\n",
    "# 提取所需信息，形成数据文件\n",
    "for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "    combined_data = pd.DataFrame()\n",
    "    data = h5py.File(file_path, 'r')\n",
    "    output_file_path = r'E:\\SETP_ICESat-2数据\\ATL_08_LandVegetation\\ATL08_ALL\\{}_{}.h5'.format(os.path.basename(file_path).split('.')[0],idx + addnum)\n",
    "    if os.path.exists(output_file_path):\n",
    "        print('{} 存在，跳过'.format(output_file_path))\n",
    "        continue\n",
    "    for subgroup in tqdm(sub_file_list, desc=\"Processing Subgroups\", leave=False):\n",
    "        if subgroup in data:\n",
    "            time_data = data.get(os.path.join(subgroup, 'land_segments/delta_time'))\n",
    "            lat = data.get(os.path.join(subgroup, 'land_segments/latitude'))\n",
    "            lon = data.get(os.path.join(subgroup, 'land_segments/longitude'))\n",
    "            height_centroid = data.get(os.path.join(subgroup, 'land_segments/canopy/centroid_height'))\n",
    "            height_canopy = data.get(os.path.join(subgroup, 'land_segments/canopy/h_canopy'))\n",
    "            dem = data.get(os.path.join(subgroup, 'land_segments/dem_h'))\n",
    "            id = data.get(os.path.join(subgroup, 'land_segments/segment_id_beg'))\n",
    "            # height = data.get(os.path.join(subgroup, 'signal_photons/ph_h'))\n",
    "            # signal_conf_ph = data.get(os.path.join(subgroup, 'signal_photons/d_flag'))# dragann flag (Meanings: [0 1]) (Values: ['noise', 'signal'])\n",
    "            cloud = data.get(os.path.join(subgroup, 'land_segments/cloud_flag_atm')) # 如果标志大于0，则可能存在气溶胶或云。有效范围为0-10\n",
    "\n",
    "            if all(x is not None for x in [time_data, lat, lon,height_centroid,height_canopy, dem ,id,cloud]):\n",
    "                # print([len(i) for i in [time_data, lat, lon,height_centroid,height_canopy, dem ,id,cloud]])\n",
    "                df = pd.DataFrame(data={\n",
    "                    'time': time_data[:],\n",
    "                    'lat': lat[:],\n",
    "                    'lon': lon[:],\n",
    "                    'height_centroid': height_centroid[:],\n",
    "                    'height_canopy': height_canopy[:],\n",
    "                    'dem': dem[:],\n",
    "                    'id':id[:],\n",
    "                    'cloud':cloud[:],\n",
    "                })\n",
    "                df['subgroup'] = subgroup\n",
    "\n",
    "                if not df.empty:\n",
    "                    # 连接属性表信息\n",
    "                    gdf_filtered = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=original_crs)\n",
    "                    \n",
    "                    batch_size = 10000\n",
    "                    n_batches = (len(gdf_filtered) + batch_size - 1) // batch_size  # 计算总批次数\n",
    "                    result = []\n",
    "\n",
    "                    for n in tqdm(range(n_batches), desc=\"Processing batches\"):\n",
    "                        gdf_batch = gdf_filtered[n * batch_size:(n + 1) * batch_size]\n",
    "                        joined_df = process_spatial_join(gdf_batch, gdf_polygons_buffered)\n",
    "                        if len(joined_df)>0:\n",
    "                            pass\n",
    "\n",
    "                        if not joined_df.empty:\n",
    "                            # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                            joined_df = joined_df.drop(columns='geometry')\n",
    "                            combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "    data.close()\n",
    "\n",
    "    if not combined_data.empty:\n",
    "        combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "    else:\n",
    "        print(f\"No data found in {file_path}\")\n",
    "        ATL08_NoData.append(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2数据\\ATL_13_InlandSurfaceWaterData', '.h5')\n",
    "file_path = file_list[0]\n",
    "data = h5py.File(file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['atl13refid', 'bottom_lat', 'bottom_lon', 'cloud_flag_asr_atl09', 'cloud_flag_atm_atl09', 'cycle_number', 'delta_time', 'err_ht_water_surf', 'err_slope_trk', 'ht_ortho', 'ht_water_surf', 'ice_flag', 'inland_water_body_id', 'inland_water_body_region', 'inland_water_body_size', 'inland_water_body_source', 'inland_water_body_type', 'layer_flag_atl09', 'met_ts_atl09', 'met_wind10_atl09', 'met_wind10_atl13', 'qf_bckgrd', 'qf_bias_em', 'qf_bias_fit', 'qf_cloud', 'qf_ht_adj', 'qf_ice', 'qf_iwp', 'qf_lseg_length', 'qf_spec_width', 'qf_sseg_length', 'qf_stdev_lseg', 'qf_stdev_vlseg', 'qf_subsurf_anomaly', 'qf_subsurface_attenuation', 'qf_subsurface_backscat_ampltd', 'rgt', 'segment_apparent_ht', 'segment_azimuth', 'segment_bias_em', 'segment_bias_fit', 'segment_dac', 'segment_dem_ht', 'segment_dem_source', 'segment_fpb_correction', 'segment_full_sat_fract', 'segment_geoid', 'segment_geoid_free2mean', 'segment_id_beg', 'segment_id_end', 'segment_lat', 'segment_lon', 'segment_near_sat_fract', 'segment_podppd_flag', 'segment_quality', 'segment_ref_elev', 'segment_slope_trk_bdy', 'segment_tide_earth_free2mean', 'segment_tide_equilibrium', 'segment_tide_ocean', 'significant_wave_ht', 'snow_ice_atl09', 'sseg_end_lat', 'sseg_end_lon', 'sseg_mean_lat', 'sseg_mean_lon', 'sseg_mean_time', 'sseg_sig_ph_cnt', 'sseg_start_lat', 'sseg_start_lon', 'stdev_water_surf', 'subsurface_attenuation', 'subsurface_backscat_ampltd', 'transect_id', 'water_depth', 'anom_ssegs']>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gt1l'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2数据\\ATL_13_InlandSurfaceWaterData', '.h5')\n",
    "ATL08_NoData = []\n",
    "# 提取所需信息，形成数据文件\n",
    "for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "    combined_data = pd.DataFrame()\n",
    "    data = h5py.File(file_path, 'r')\n",
    "    output_file_path = r'E:\\SETP_ICESat-2数据\\ATL_13_InlandSurfaceWaterData\\ATL13_ALL\\{}_{}.h5'.format(os.path.basename(file_path).split('.')[0],idx + addnum)\n",
    "    if os.path.exists(output_file_path):\n",
    "        print('{} 存在，跳过'.format(output_file_path))\n",
    "        continue\n",
    "    for subgroup in tqdm(sub_file_list, desc=\"Processing Subgroups\", leave=False):\n",
    "        if subgroup in data:\n",
    "            time_data = data.get(os.path.join(subgroup, 'delta_time'))\n",
    "            lat = data.get(os.path.join(subgroup, 'bottom_lat'))\n",
    "            segment_lat = data.get(os.path.join(subgroup, 'segment_lat'))\n",
    "            lon = data.get(os.path.join(subgroup, 'bottom_lon'))\n",
    "            segment_lon = data.get(os.path.join(subgroup, 'segment_lon'))\n",
    "            height_surface = data.get(os.path.join(subgroup, 'ht_water_surf'))\n",
    "            water_depth = data.get(os.path.join(subgroup, 'water_depth'))\n",
    "            dem = data.get(os.path.join(subgroup, 'segment_dem_ht'))\n",
    "            id = data.get(os.path.join(subgroup, 'segment_id_beg'))\n",
    "            # Cloud probability from ASR.; (Meanings: [0 1 2 3 4 5]) \n",
    "            # (Values: ['clear_with_high_confidence', 'clear_with_medium_confidence', 'clear_with_low_confidence', \n",
    "            # 'cloudy_with_low_confidence', 'cloudy_with_medium_confidence', 'cloudy_with_high_confidence'])\n",
    "            cloud = data.get(os.path.join(subgroup, 'cloud_flag_asr_atl09')) \n",
    "            ice_flag = data.get(os.path.join(subgroup, 'ice_flag'))\n",
    "            inland_water_body_type = data.get(os.path.join(subgroup, 'inland_water_body_type'))\n",
    "\n",
    "            if all(x is not None for x in [time_data, lat,segment_lat, lon,segment_lon,height_surface,water_depth, dem ,id,cloud,ice_flag,inland_water_body_type]):\n",
    "                # print([len(i) for i in [time_data, lat, lon,height_centroid,height_canopy, dem ,id,cloud]])\n",
    "                df = pd.DataFrame(data={\n",
    "                    'time': time_data[:],\n",
    "                    'lat': lat[:],\n",
    "                    'segment_lat': segment_lat[:],\n",
    "                    'lon': lon[:],\n",
    "                    'segment_lon': segment_lon[:],\n",
    "                    'height_surface': height_surface[:],\n",
    "                    'water_depth': water_depth[:],\n",
    "                    'dem': dem[:],\n",
    "                    'id':id[:],\n",
    "                    'cloud':cloud[:],\n",
    "                    'ice_flag':ice_flag[:],\n",
    "                    'inland_water_body_type':inland_water_body_type[:]\n",
    "                })\n",
    "                df['subgroup'] = subgroup\n",
    "\n",
    "                if not df.empty:\n",
    "                    # 连接属性表信息\n",
    "                    gdf_filtered = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=original_crs)\n",
    "                    \n",
    "                    batch_size = 10000\n",
    "                    n_batches = (len(gdf_filtered) + batch_size - 1) // batch_size  # 计算总批次数\n",
    "                    result = []\n",
    "\n",
    "                    for n in tqdm(range(n_batches), desc=\"Processing batches\"):\n",
    "                        gdf_batch = gdf_filtered[n * batch_size:(n + 1) * batch_size]\n",
    "                        joined_df = process_spatial_join(gdf_batch, gdf_polygons_buffered)\n",
    "                        if len(joined_df)>0:\n",
    "                            pass\n",
    "\n",
    "                        if not joined_df.empty:\n",
    "                            # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                            joined_df = joined_df.drop(columns='geometry')\n",
    "                            combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "    data.close()\n",
    "\n",
    "    if not combined_data.empty:\n",
    "        combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "    else:\n",
    "        print(f\"No data found in {file_path}\")\n",
    "        ATL08_NoData.append(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
