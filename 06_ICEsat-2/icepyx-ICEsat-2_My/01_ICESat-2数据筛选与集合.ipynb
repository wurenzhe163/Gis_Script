{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import geopandas as gpd\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.geometry import Point\n",
    "from PackageDeepLearn.utils import file_search_wash as fsw\n",
    "\n",
    "# 所有条带\n",
    "sub_file_list = ['gt1l/', 'gt1r/', 'gt2l/', 'gt2r/', 'gt3l/', 'gt3r/']\n",
    "start_time = time.time()\n",
    "\n",
    "shapefile_path = r\"D:\\BaiduSyncdisk\\02_论文相关\\在写\\SAM冰湖\\数据\\2023_05_31_to_2023_09_15_样本修正.shp\"\n",
    "gdf_polygons = gpd.read_file(shapefile_path)\n",
    "\n",
    "# 假设原始的 CRS 是 EPSG:4326\n",
    "original_crs = gdf_polygons.crs\n",
    "\n",
    "# 转换为适当的投影坐标系，例如 EPSG:8859 eq earth asia\n",
    "projected_gdf = gdf_polygons.to_crs(epsg=8859)\n",
    "\n",
    "# 应用50米的缓冲区\n",
    "projected_gdf['geometry'] = projected_gdf.geometry.buffer(50)\n",
    "\n",
    "# 将结果转换回原始的地理坐标系\n",
    "gdf_polygons_buffered = projected_gdf.to_crs(original_crs)\n",
    "\n",
    "def process_spatial_join(gdf_batch, gdf_polygons_buffered):\n",
    "    # 执行空间连接\n",
    "    joined_df = gpd.sjoin(gdf_batch, gdf_polygons_buffered, how='left', predicate='intersects')\n",
    "\n",
    "    # 确保只有那些与目标几何体相交的条目被保留\n",
    "    if 'index_right' in joined_df.columns:\n",
    "        joined_df = joined_df.dropna(subset=['index_right'])\n",
    "\n",
    "    return joined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_03 \n",
    "属性表直接连接筛选，快速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14c1e570c8a4e8e9233a5c223c30af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files:   0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing h5 file: E:\\SETP_ICESat-2\\ATL_03_GlobalGeolocatedPhoton\\ATL03_20221231195214_01721806_006_02.h5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495e5003f7cd4694a3498e614bac529f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Subgroups:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing subgroup: gt1l/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5b24b797954184bbcf02220c0c9775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing subgroup: gt1r/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13f8e499a9243b3addba144c5c92aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing subgroup: gt2l/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180360a9c6194a12b0c13f82c89789f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2\\ATL_03_GlobalGeolocatedPhoton', '.h5')\n",
    "# 提取所需信息，形成数据文件\n",
    "for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "    tqdm.write(f'Processing h5 file: {file_path}')\n",
    "    combined_data = pd.DataFrame()\n",
    "    data = h5py.File(file_path, 'r')\n",
    "    output_file_path = r'E:\\SETP_ICESat-2\\ATL_03_GlobalGeolocatedPhoton\\ATL03_ALL\\ATL03_SETPGL_ALL_{}.h5'.format(idx)\n",
    "    if os.path.exists(output_file_path):\n",
    "        print('{} 存在，跳过'.format(output_file_path))\n",
    "        continue\n",
    "    for subgroup in tqdm(sub_file_list, desc=\"Processing Subgroups\", leave=False):\n",
    "        tqdm.write(f'  Processing subgroup: {subgroup}')\n",
    "        if subgroup in data:\n",
    "            time_data = data.get(os.path.join(subgroup, 'heights/delta_time'))\n",
    "            lat = data.get(os.path.join(subgroup, 'heights/lat_ph'))\n",
    "            lon = data.get(os.path.join(subgroup, 'heights/lon_ph'))\n",
    "            dist_ph_along = data.get(os.path.join(subgroup, 'heights/dist_ph_along'))\n",
    "            height = data.get(os.path.join(subgroup, 'heights/h_ph'))\n",
    "            signal_conf_ph = data.get(os.path.join(subgroup, 'heights/signal_conf_ph'))\n",
    "            quality_ph = data.get(os.path.join(subgroup, 'heights/quality_ph'))\n",
    "\n",
    "            if all(x is not None for x in [lat, lon, height, time_data, dist_ph_along, quality_ph, signal_conf_ph]):\n",
    "                df = pd.DataFrame(data={\n",
    "                    'time': time_data[:],\n",
    "                    'lat': lat[:],\n",
    "                    'lon': lon[:],\n",
    "                    'dist_ph_along': dist_ph_along[:],\n",
    "                    'height': height[:],\n",
    "                    'quality_ph': quality_ph[:],\n",
    "                    'signal_conf_ph_1': signal_conf_ph[:, 0],\n",
    "                    'signal_conf_ph_2': signal_conf_ph[:, 1],\n",
    "                    'signal_conf_ph_3': signal_conf_ph[:, 2],\n",
    "                    'signal_conf_ph_4': signal_conf_ph[:, 3],\n",
    "                    'signal_conf_ph_5': signal_conf_ph[:, 4]\n",
    "                })\n",
    "                \n",
    "                df['subgroup'] = subgroup\n",
    "                \n",
    "                # 过滤数据，删除 signal_conf_ph_1、signal_conf_ph_2、signal_conf_ph_3、signal_conf_ph_4、signal_conf_ph_5 小于 0 的数据\n",
    "                df = df[(df['signal_conf_ph_1'] >= 2) |\n",
    "                        (df['signal_conf_ph_4'] >= 2) |\n",
    "                        (df['signal_conf_ph_5'] >= 2)]\n",
    "                \n",
    "\n",
    "                if not df.empty:\n",
    "                    # 连接属性表信息\n",
    "                    gdf_filtered = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=original_crs)\n",
    "                    \n",
    "                    batch_size = 10000\n",
    "                    n_batches = (len(gdf_filtered) + batch_size - 1) // batch_size  # 计算总批次数\n",
    "                    result = []\n",
    "\n",
    "                    for n in tqdm(range(n_batches), desc=\"Processing batches\",leave=False):\n",
    "                        gdf_batch = gdf_filtered[n * batch_size:(n + 1) * batch_size]\n",
    "                        joined_df = process_spatial_join(gdf_batch, gdf_polygons_buffered)\n",
    "                        if len(joined_df)>0:\n",
    "                            pass\n",
    "\n",
    "                        if not joined_df.empty:\n",
    "                            # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                            joined_df = joined_df.drop(columns='geometry')\n",
    "                            combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "    data.close()\n",
    "\n",
    "    if not combined_data.empty:\n",
    "        combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2\\ATL_06_Landice', '.h5')\n",
    "ATL06_NoData = []\n",
    "# 提取所需信息，形成数据文件\n",
    "try:\n",
    "    for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "        combined_data = pd.DataFrame()\n",
    "        data = h5py.File(file_path, 'r')\n",
    "        output_file_path = r'E:\\SETP_ICESat-2\\ATL_06_Landice\\ATL06_ALL\\{}_{}.h5'.format(os.path.basename(file_path).split('.')[0],idx + addnum)\n",
    "        if os.path.exists(output_file_path):\n",
    "            print('{} 存在，跳过'.format(output_file_path))\n",
    "            continue\n",
    "        for subgroup in tqdm(sub_file_list, desc=\"Processing Subgroups\", leave=False):\n",
    "            if subgroup in data:\n",
    "                time_data = data.get(os.path.join(subgroup, 'land_ice_segments/delta_time'))\n",
    "                lat = data.get(os.path.join(subgroup, 'land_ice_segments/latitude'))\n",
    "                lon = data.get(os.path.join(subgroup, 'land_ice_segments/longitude'))\n",
    "                height = data.get(os.path.join(subgroup, 'land_ice_segments/h_li'))  \n",
    "                dem = data.get(os.path.join(subgroup, 'land_ice_segments/dem/dem_h'))\n",
    "                signal_conf_ph = data.get(os.path.join(subgroup, 'land_ice_segments/atl06_quality_summary')) #(Meanings: [0 1]) (Values: ['best_quality', 'potential_problem'])\n",
    "                id = data.get(os.path.join(subgroup, 'land_ice_segments/segment_id'))      \n",
    "\n",
    "                if all(x is not None for x in [time_data, lat, lon, height,dem, signal_conf_ph ,id]):\n",
    "                    df = pd.DataFrame(data={\n",
    "                        'time': time_data[:],\n",
    "                        'lat': lat[:],\n",
    "                        'lon': lon[:],\n",
    "                        'height': height[:],\n",
    "                        'dem':dem,\n",
    "                        'signal_conf_ph':signal_conf_ph[:],\n",
    "                        'id':id[:],\n",
    "                    })\n",
    "                    df['subgroup'] = subgroup\n",
    "\n",
    "                    if not df.empty:\n",
    "                        # 连接属性表信息\n",
    "                        gdf_filtered = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=original_crs)\n",
    "                        \n",
    "                        batch_size = 10000\n",
    "                        n_batches = (len(gdf_filtered) + batch_size - 1) // batch_size  # 计算总批次数\n",
    "                        result = []\n",
    "\n",
    "                        for n in tqdm(range(n_batches), desc=\"Processing batches\"):\n",
    "                            gdf_batch = gdf_filtered[n * batch_size:(n + 1) * batch_size]\n",
    "                            joined_df = process_spatial_join(gdf_batch, gdf_polygons_buffered)\n",
    "                            if len(joined_df)>0:\n",
    "                                pass\n",
    "\n",
    "                            if not joined_df.empty:\n",
    "                                # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                                joined_df = joined_df.drop(columns='geometry')\n",
    "                                combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "        data.close()\n",
    "\n",
    "        if not combined_data.empty:\n",
    "            combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "        else:\n",
    "            print(f\"No data found in {file_path}\")\n",
    "            ATL06_NoData.append(file_path)\n",
    "\n",
    "except:\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2\\ATL_07_Seaice', '.h5')\n",
    "ATL07_NoData = []\n",
    "# 提取所需信息，形成数据文件\n",
    "for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "    combined_data = pd.DataFrame()\n",
    "    data = h5py.File(file_path, 'r')\n",
    "    output_file_path = r'E:\\SETP_ICESat-2\\ATL_07_Seaice\\ATL07_ALL\\{}_{}.h5'.format(os.path.basename(file_path).split('.')[0],idx)\n",
    "    if os.path.exists(output_file_path):\n",
    "        print('{} 存在，跳过'.format(output_file_path))\n",
    "        continue\n",
    "    for subgroup in sub_file_list:\n",
    "        if subgroup in data:\n",
    "            time_data = data.get(os.path.join(subgroup, 'sea_ice_segments/delta_time'))\n",
    "            lat = data.get(os.path.join(subgroup, 'sea_ice_segments/latitude'))\n",
    "            lon = data.get(os.path.join(subgroup, 'sea_ice_segments/longitude'))\n",
    "            height = data.get(os.path.join(subgroup, 'sea_ice_segments/heights/height_segment_height'))\n",
    "            dem = data.get(os.path.join(subgroup, 'dem/dem_h'))\n",
    "            id = data.get(os.path.join(subgroup, 'sea_ice_segments/height_segment_id'))\n",
    "            signal_conf_ph = data.get(os.path.join(subgroup, 'sea_ice_segments/heights/height_segment_quality'))# (Meanings: [0 1]) (Values: ['bad_quality', 'good_quality'])\n",
    "\n",
    "            if all(x is not None for x in [time_data, lat, lon, height, dem, signal_conf_ph ,id]):\n",
    "                print([len(i) for i in [time_data, lat, lon, height, dem, signal_conf_ph ,id]])\n",
    "                df = pd.DataFrame(data={\n",
    "                    'time': time_data[:],\n",
    "                    'lat': lat[:],\n",
    "                    'lon': lon[:],\n",
    "                    'height': height[:],\n",
    "                    'dem': dem[:],\n",
    "                    'signal_conf_ph':signal_conf_ph[:],\n",
    "                    'id':id[:],\n",
    "                })\n",
    "                df['subgroup'] = subgroup\n",
    "\n",
    "                if not df.empty:\n",
    "                    # 连接属性表信息\n",
    "                    gdf_filtered = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=original_crs)\n",
    "                    \n",
    "                    batch_size = 10000\n",
    "                    n_batches = (len(gdf_filtered) + batch_size - 1) // batch_size  # 计算总批次数\n",
    "                    result = []\n",
    "\n",
    "                    for n in range(n_batches):\n",
    "                        gdf_batch = gdf_filtered[n * batch_size:(n + 1) * batch_size]\n",
    "                        joined_df = process_spatial_join(gdf_batch, gdf_polygons_buffered)\n",
    "                        if len(joined_df)>0:\n",
    "                            pass\n",
    "\n",
    "                        if not joined_df.empty:\n",
    "                            # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                            joined_df = joined_df.drop(columns='geometry')\n",
    "                            combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "    data.close()\n",
    "\n",
    "    if not combined_data.empty:\n",
    "        combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "    else:\n",
    "        print(f\"No data found in {file_path}\")\n",
    "        ATL07_NoData.append(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2数据\\ATL_08_LandVegetation', '.h5')\n",
    "ATL08_NoData = []\n",
    "# 提取所需信息，形成数据文件\n",
    "for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "    combined_data = pd.DataFrame()\n",
    "    data = h5py.File(file_path, 'r')\n",
    "    output_file_path = r'E:\\SETP_ICESat-2数据\\ATL_08_LandVegetation\\ATL08_ALL\\{}_{}.h5'.format(os.path.basename(file_path).split('.')[0],idx )\n",
    "    if os.path.exists(output_file_path):\n",
    "        print('{} 存在，跳过'.format(output_file_path))\n",
    "        continue\n",
    "    for subgroup in tqdm(sub_file_list, desc=\"Processing Subgroups\", leave=False):\n",
    "        if subgroup in data:\n",
    "            time_data = data.get(os.path.join(subgroup, 'land_segments/delta_time'))\n",
    "            lat = data.get(os.path.join(subgroup, 'land_segments/latitude'))\n",
    "            lon = data.get(os.path.join(subgroup, 'land_segments/longitude'))\n",
    "            height_centroid = data.get(os.path.join(subgroup, 'land_segments/canopy/centroid_height'))\n",
    "            height_canopy = data.get(os.path.join(subgroup, 'land_segments/canopy/h_canopy'))\n",
    "            dem = data.get(os.path.join(subgroup, 'land_segments/dem_h'))\n",
    "            id = data.get(os.path.join(subgroup, 'land_segments/segment_id_beg'))\n",
    "            # height = data.get(os.path.join(subgroup, 'signal_photons/ph_h'))\n",
    "            # signal_conf_ph = data.get(os.path.join(subgroup, 'signal_photons/d_flag'))# dragann flag (Meanings: [0 1]) (Values: ['noise', 'signal'])\n",
    "            cloud = data.get(os.path.join(subgroup, 'land_segments/cloud_flag_atm')) # 如果标志大于0，则可能存在气溶胶或云。有效范围为0-10\n",
    "\n",
    "            if all(x is not None for x in [time_data, lat, lon,height_centroid,height_canopy, dem ,id,cloud]):\n",
    "                # print([len(i) for i in [time_data, lat, lon,height_centroid,height_canopy, dem ,id,cloud]])\n",
    "                df = pd.DataFrame(data={\n",
    "                    'time': time_data[:],\n",
    "                    'lat': lat[:],\n",
    "                    'lon': lon[:],\n",
    "                    'height_centroid': height_centroid[:],\n",
    "                    'height_canopy': height_canopy[:],\n",
    "                    'dem': dem[:],\n",
    "                    'id':id[:],\n",
    "                    'cloud':cloud[:],\n",
    "                })\n",
    "                df['subgroup'] = subgroup\n",
    "\n",
    "                if not df.empty:\n",
    "                    # 连接属性表信息\n",
    "                    gdf_filtered = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=original_crs)\n",
    "                    \n",
    "                    batch_size = 10000\n",
    "                    n_batches = (len(gdf_filtered) + batch_size - 1) // batch_size  # 计算总批次数\n",
    "                    result = []\n",
    "\n",
    "                    for n in tqdm(range(n_batches), desc=\"Processing batches\"):\n",
    "                        gdf_batch = gdf_filtered[n * batch_size:(n + 1) * batch_size]\n",
    "                        joined_df = process_spatial_join(gdf_batch, gdf_polygons_buffered)\n",
    "                        if len(joined_df)>0:\n",
    "                            pass\n",
    "\n",
    "                        if not joined_df.empty:\n",
    "                            # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                            joined_df = joined_df.drop(columns='geometry')\n",
    "                            combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "    data.close()\n",
    "\n",
    "    if not combined_data.empty:\n",
    "        combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "    else:\n",
    "        print(f\"No data found in {file_path}\")\n",
    "        ATL08_NoData.append(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATL_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2\\ATL_13_InlandSurfaceWaterData', '.h5')\n",
    "file_path = file_list[0]\n",
    "data = h5py.File(file_path, 'r')\n",
    "for key in data['gt1l'].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = fsw.search_files(r'E:\\SETP_ICESat-2\\ATL_13_InlandSurfaceWaterData', '.h5')\n",
    "ATL13_NoData = []\n",
    "# 提取所需信息，形成数据文件\n",
    "for idx, file_path in enumerate(tqdm(file_list, desc=\"Processing Files\")):\n",
    "    combined_data = pd.DataFrame()\n",
    "    data = h5py.File(file_path, 'r')\n",
    "    output_file_path = r'E:\\SETP_ICESat-2\\ATL_13_InlandSurfaceWaterData\\ATL13_ALL\\{}_{}.h5'.format(os.path.basename(file_path).split('.')[0],idx)\n",
    "    if os.path.exists(output_file_path):\n",
    "        print('{} 存在，跳过'.format(output_file_path))\n",
    "        continue\n",
    "    for subgroup in tqdm(sub_file_list, desc=\"Processing Subgroups\", leave=False):\n",
    "        if subgroup in data:\n",
    "            time_data = data.get(os.path.join(subgroup, 'delta_time'))\n",
    "            lat = data.get(os.path.join(subgroup, 'bottom_lat'))\n",
    "            segment_lat = data.get(os.path.join(subgroup, 'segment_lat'))\n",
    "            lon = data.get(os.path.join(subgroup, 'bottom_lon'))\n",
    "            segment_lon = data.get(os.path.join(subgroup, 'segment_lon'))\n",
    "            height_surface = data.get(os.path.join(subgroup, 'ht_water_surf'))\n",
    "            water_depth = data.get(os.path.join(subgroup, 'water_depth'))\n",
    "            dem = data.get(os.path.join(subgroup, 'segment_dem_ht'))\n",
    "            id = data.get(os.path.join(subgroup, 'segment_id_beg'))\n",
    "            # Cloud probability from ASR.; (Meanings: [0 1 2 3 4 5]) \n",
    "            # (Values: ['clear_with_high_confidence', 'clear_with_medium_confidence', 'clear_with_low_confidence', \n",
    "            # 'cloudy_with_low_confidence', 'cloudy_with_medium_confidence', 'cloudy_with_high_confidence'])\n",
    "            cloud = data.get(os.path.join(subgroup, 'cloud_flag_asr_atl09')) \n",
    "            ice_flag = data.get(os.path.join(subgroup, 'ice_flag'))\n",
    "            inland_water_body_type = data.get(os.path.join(subgroup, 'inland_water_body_type'))\n",
    "\n",
    "            if all(x is not None for x in [time_data, lat,segment_lat, lon,segment_lon,height_surface,water_depth, dem ,id,cloud,ice_flag,inland_water_body_type]):\n",
    "                # print([len(i) for i in [time_data, lat, lon,height_centroid,height_canopy, dem ,id,cloud]])\n",
    "                df = pd.DataFrame(data={\n",
    "                    'time': time_data[:],\n",
    "                    'lat': lat[:],\n",
    "                    'segment_lat': segment_lat[:],\n",
    "                    'lon': lon[:],\n",
    "                    'segment_lon': segment_lon[:],\n",
    "                    'height_surface': height_surface[:],\n",
    "                    'water_depth': water_depth[:],\n",
    "                    'dem': dem[:],\n",
    "                    'id':id[:],\n",
    "                    'cloud':cloud[:],\n",
    "                    'ice_flag':ice_flag[:],\n",
    "                    'inland_water_body_type':inland_water_body_type[:]\n",
    "                })\n",
    "                df['subgroup'] = subgroup\n",
    "\n",
    "                if not df.empty:\n",
    "                    # 连接属性表信息\n",
    "                    gdf_filtered = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']), crs=original_crs)\n",
    "                    \n",
    "                    batch_size = 10000\n",
    "                    n_batches = (len(gdf_filtered) + batch_size - 1) // batch_size  # 计算总批次数\n",
    "                    result = []\n",
    "\n",
    "                    for n in tqdm(range(n_batches), desc=\"Processing batches\"):\n",
    "                        gdf_batch = gdf_filtered[n * batch_size:(n + 1) * batch_size]\n",
    "                        joined_df = process_spatial_join(gdf_batch, gdf_polygons_buffered)\n",
    "                        if len(joined_df)>0:\n",
    "                            pass\n",
    "\n",
    "                        if not joined_df.empty:\n",
    "                            # 移除 'geometry' 列，以便合并到 combined_data 中\n",
    "                            joined_df = joined_df.drop(columns='geometry')\n",
    "                            combined_data = pd.concat([combined_data, joined_df], ignore_index=True)\n",
    "    data.close()\n",
    "\n",
    "    if not combined_data.empty:\n",
    "        combined_data.to_hdf(output_file_path, key='df', mode='w')\n",
    "    else:\n",
    "        print(f\"No data found in {file_path}\")\n",
    "        ATL13_NoData.append(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
